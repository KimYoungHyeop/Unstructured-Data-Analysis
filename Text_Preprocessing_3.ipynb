{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Preprocessing_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## < Research Trends in NLP >"
      ],
      "metadata": {
        "id": "eFdjuGDxhpLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From rule-based approaches to statistical approaches\n",
        "- The classical way: until late 1980's  \n",
        "Rule-based approaches:\n",
        "    - are too rigid for natural language\n",
        "    - suffer from the *knowledge acquisition bottleneck*\n",
        "    - cannot keep up with changing/evolving language  \n",
        "     *ex. \"to google\"*   \n",
        "\n",
        "- The statistical way: since early 1990's\n",
        "\"Statistical NLP\" refers to all quantitative approaches, including Bayes' models, Hidden Markov Models (HMMs), Support Vector Machines (SVMs), Clustering, ...\n",
        "    - more robust & more flexible\n",
        "    - need a *Corpus* for (supervised of unsupervised) learning But real-world systems typically combine both."
      ],
      "metadata": {
        "id": "KLXBGayNhpDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From statistical approaches to machine-learning (deep-learning) approaches  \n",
        "\n",
        "rule-based는 사실 \"연역적 사고를 갖고 있는 자연어 처리 엔진을 만들자\"라는 의도였다면 머신러닝 기반으로 바뀌면서 \"귀납적 사고의 자연어 처리 엔진을 만들자\"로 헤게모니 바뀌었습니다.  \n",
        "\n",
        "rule은 결국 사람이 정해주는 것이므로 그 룰로부터 sub-rule을 파생할 수 있고 그런 symbolic 연산을 할 수 있는 엔진과   \n",
        "단지 데이터만 충분히 주고 상황별 결과물을 바탕으로 둘 사이의 logic을 역으로 추정하게 해서 방향성은 서로 정반대라고 할 수 있습니다."
      ],
      "metadata": {
        "id": "9eHul12ThpGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### End-to-End Multi-Task Learning  \n",
        "\n",
        "핵심은 'End-to-End'와 'Multi-task'입니다.   \n",
        "\n",
        "과거에 Classical NLP에서는 Documents가 주어지면 이 Documents를 바탕으로 해서 Language Detection을 하고, Pre-processing하는데도 Domain knowledge가 들어가고, 어떻게 Modeling을 하는데에도 Domain knowledge가 들어갔습니다. 따라서 사람의 개입이 들어갔었지만\n",
        "\n",
        "Deep Learning-based NLP에서는 \"Documents와 최종적인 output에 대한 label들만 주면 다양한 task들을 한꺼번에 수행할 수 있는 종단 학습(처음부터 끝까지 사람의 개입 필요x)이 가능한 자연어 처리 엔진을 만들겠다\"가 최근에 자연어 처리의 trend중 하나라고 보면 될 것 같습니다."
      ],
      "metadata": {
        "id": "6tGyAAhDhpIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Improvements  \n",
        "\n",
        "실질적으로 자연어 처리 관점에서 성능 향상도를 보면 Representation Learning, Machine Translation, Question Answering, Language Modeling 등의 대부분의 방법론은 전부 최근에 나온 Pre-training model을 기반으로 해서 개선을 했거나 딥러닝 기반으로 앙상블을 했거나 입니다.   \n",
        "\n",
        "rule-based 방법론, 통계적 기반의 방법론들은 이제 거의 찾아보기 힘듭니다."
      ],
      "metadata": {
        "id": "UzGvH9mRhpNn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10 Exiting ideas of 2018 in NLP\n",
        "- Unsupervised Machine Traslation\n",
        "- Pretrained language models\n",
        "- Common sense inference datasets\n",
        "- Meta-learning\n",
        "- Robust unsupervised methods\n",
        "- Understanding representations\n",
        "- Clever auxiliary tasks\n",
        "- Combining semi-supervised learning with transfer learning\n",
        "- QA and reasoning with large documents\n",
        "- Inductive bias"
      ],
      "metadata": {
        "id": "naG9LAeXhpSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Major NLP Achievements & Papers From 2019\n",
        "- Language Models Are Unsupervised Multitask Learners\n",
        "- XLNet: Generalized Autoregressive Pretraining for Language Understanding\n",
        "- RoBERTa: A Robustly Optimized BERT Pretraining Approach\n",
        "- Emotion-Cause Pair Extraction: A New Task to Emotion Analysis in Texts\n",
        "- Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems\n",
        "- Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks\n",
        "- Probing the Need for Visual Context in Multimodal Machine Translation\n",
        "- Bridging the Gap between Training and Inference for Neural Machine Translation\n",
        "- On Extractive and Abstractive Neural Document Summarization with Transformer Language Models\n",
        "- CTRL: A Conditional Transformer Language Model For Controllable Generation\n",
        "- ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
      ],
      "metadata": {
        "id": "yxll-aKuhpUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14 NLP Research Breakthroughs You Can Apply To Your Business\n",
        "- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
        "- Sequence Classification with Human Attention\n",
        "- Phrase-Based & Neural Unsupervised Machine Translation\n",
        "- What you can cram into a single vector: Probing sentence embeddings for linguistic properties\n",
        "- SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference\n",
        "- Deep contextualized word representations\n",
        "- Meta-Learning for Low-Resource Neural Machine Translation\n",
        "- Linguistically-Informed Self-Attention for Semantic Role Labeling\n",
        "- A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks\n",
        "- Know What You Don’t Know: Unanswerable Questions for SQuAD\n",
        "- An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling\n",
        "- Universal Language Model Fine-tuning for Text Classification\n",
        "- Improving Language Understanding by Generative Pre-Training\n",
        "- Dissecting Contextual Word Embeddings: Architecture and Representation"
      ],
      "metadata": {
        "id": "Dyefu6a_hpWw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbWpvBmehmwl"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}