{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Preprocessing_7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## < Lexical Analysis 4: Part-of-Speech (POS) Tagging>"
      ],
      "metadata": {
        "id": "DsMDPyGtPjN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pointwise Prediction: **Maximum Entropy Model**  \n",
        "- Encode features for tag prediction\n",
        "    - Information about word/context: suffix, prefix, neighborhood word  information  \n",
        "  설명: Pointwise Prediction이 어떻게 작동하는지에 대한 간단한 설명을 해보겠습니다. 우선 tag prediction을 하기 위해 features encoding을 진행하게 됩니다. 여기서 features encoding 이란, 특정한 word와 context 상의 정보를 가지고 봤을 때 이 정보가 특정 품사를 예측하는 데에 매우 중요한 역할을 할지 말지에 해당하는 정보를 담고 있습니다.(suffix, prefix, neighborhood word, ...)\n",
        "- Tagging Model  \n",
        "  $$p(t|C) = \\frac{1}{Z(C)}exp(\\sum_{i=1}^n\t\n",
        "\\lambda_if_i(C,t))$$  \n"
      ],
      "metadata": {
        "id": "XduhipBIPjQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- $f_i$ is a feature\n",
        "- $λ_i$ is a weight (large value implies informative featureds)\n",
        "- $Z(C)$ is a normalization constant ensuring a proper probability distribution\n",
        "- Makes no independence assumption about the features   \n",
        "\n",
        "  설명: 요즘은 이런 것을 handcrafted feature라고 해서 지양하는 분위기이고, End-to-end learning으로 사람의 개입을 없애는 방향을 선호합니다. 다만, 언어학적인 특징을 가지고 봤을 때 특정한 상황에서는 어떤 품사인지가 명확히 정의될 수 있다면 이러한 feature를 사용해서 rule-based 또는 결합을 통한 모형으로 사용하는 것은 괜찮습니다. "
      ],
      "metadata": {
        "id": "JRvIJpKyPk7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Probabilistic Model for POS Tagging\n",
        "- Find the **most probable tag sequence** given the sentence  \n",
        "\n",
        "  Ex) Natural**(JJ)** language**(NN)** processing**(NN)** **((LRB)**NLP**(NN)**)**(RRB)** is**(VBZ)** a**(DT)** field**(NN)** of**(IN)** computer**(NN)** science**(NN)**   \n",
        "\n",
        "  $$argmax P(Y|X)$$"
      ],
      "metadata": {
        "id": "CDc3LOPHPk99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generative Sequence Model\n",
        "- Decompose probability using Baye's Rule  \n",
        "\n",
        "  $$argmax P(Y|X) = argmax\\frac{P(X|Y)P(Y)}{P(X)} = argmax P(X|Y)P(Y)$$"
      ],
      "metadata": {
        "id": "gov6GfZOPlAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generative Sequence Model: Hidden Markov Model\n",
        "- <center>POS ➡ POS transition probabilities</center>  \n",
        "\n",
        "$$P(Y) \\approx \\Pi_{i=1}^{l+1}P_T(y_i|y_{i-1})$$  \n",
        "\n",
        "- <center>OS ➡ Word emission probabilities</center>   \n",
        "\n",
        "$$P(X|Y) \\approx \\Pi_{1}^{l}P_E(X_i|Y_i)$$"
      ],
      "metadata": {
        "id": "JoSDJyAOPjSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discriminative Sequence Model: Conditional Random Field (CRF)\n",
        "- Relieve that constraint that a tag is generated by the previous tag sequence\n",
        "- Predict the whole tag set at the same time, not sequentially"
      ],
      "metadata": {
        "id": "Aw_UyMaVPjVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Network-based Models\n",
        "- Window-based vs. sentence-based\n",
        "- Recurrent neural networks: have a feedback loop within the hidden layer\n",
        "- Input-Output mapping of RNNs"
      ],
      "metadata": {
        "id": "oc-XmlrsPjXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hybrid model: LSTM(RNN) + ConvNet + CRF  \n",
        "\n",
        "설명: Hybrid model입니다. 단어의 처리는 RNN 계열의 LSTM으로 하고, 제일 윗단에 CRF Layer를 둬서 Neural network 기반의 방법론과 고전적인 방식인 CRF를 결합해 조금 더 performance를 높이는 것도 하나의 시도로써 기억할 수 있겠습니다. "
      ],
      "metadata": {
        "id": "rjnANIWLPjai"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMAyBRCkPh3k"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}