{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Preprocessing_10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Other topics in NLP"
      ],
      "metadata": {
        "id": "PNxIUWfcYOYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## < Language Modeling >  \n",
        "### Probabilistic Language Model\n",
        "- Assign a probability to a sentence (not POS tags, but the sentence itself)  \n",
        "\n",
        "### Applications\n",
        "- Machine Translation\n",
        "    - P(**high** wind tonight) > P(**large** wind tonight) \n",
        "- Spell correction\n",
        "    - The office is about fifteen **minuets** from my house\n",
        "    - P(about fifteen **minutes** from) > P(about fifteen minuets from)\n",
        "- Speech recognition\n",
        "    - P(I saw a van) >> P(eyes awe of an)\n",
        "- Summarization, question-answering, etc."
      ],
      "metadata": {
        "id": "WVCL-7zIYPNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Probabilistic Languege Modeling\n",
        "- Compute the probability of a sentence or sequence of words  \n",
        "$$P(W) = P(w_1, w_2, w_3, ... w_n)$$\n",
        "\n",
        "- Related task: probability of an upcoming word  \n",
        "$$P(w_5|w_1, w_2, w_3, w_4)$$  \n",
        "\n",
        "    - ex) I love you more than i can ____. (swim? say?)"
      ],
      "metadata": {
        "id": "aE4ZDvffYPQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to comput P(W)\n",
        "- What is P(its, water, is, so, transparent, that)?\n",
        "- Chain Rules of probability:  \n",
        "\n",
        "$$P(w_1, w_2, w_3, ... w_n) = P(w_1)P(w_2|w_1)P(w_3|w_1, w_2)...P(w_n|w_1, ..., W_{n-1})$$   \n",
        "\n",
        "\n",
        "  $$P(\"its water is so transparent\") = P(its) × (water|its) × P(is|its water) × P(so|its water is) × P(transparent|its water is so)$$"
      ],
      "metadata": {
        "id": "_f9pS2VFYPTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Markov Assumption\n",
        "- Consider only k previous words when estimating the conditional probability  \n",
        "\n",
        "$$P(w_1, w_2... w_n) \\approx Π  P(w_i|w_{i-k}...w_{i-1})$$ \n",
        "$$P(w_i|w_1w_2... w_{i-1}) \\approx Π  P(w_i|w_{i-k}...w_{i-1})$$   \n",
        "\n",
        "- Simplest case: Unigram model  \n",
        "\n",
        "$$P(w_1w_2...w_n) \\approx Π  P(w_i)$$   \n",
        "\n",
        "부연) 뒷부분을 없애버리고 그냥 유니그램 모델로 만들자는 것입니다.   \n",
        "즉, 해당하는 LM은 각각의 단어가 독립적으로 발생했다를 가정하자는 의미입니다.\n",
        "\n",
        "- An example of automatically generated sentences from a unigram model  \n",
        "\n"
      ],
      "metadata": {
        "id": "L8mUm_XjYPWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> *fifth, an, of, futures, the, an, incorporated, a, a, the, inflation, most, dollars, quarter, in, is, mass*  \n",
        "*thrift, did, eighty, said, hard, 'm, july, bullish*  \n",
        "*that, or, limited, the*\n"
      ],
      "metadata": {
        "id": "7eEDnvxLYPYl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SA-AezhYM_T"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}